"""
@file regressao-linear-ex1.py
@brief Exercise 2 - Linear Regression implementation with visualization.

This script performs the following tasks:
1. Runs a warm-up exercise.
2. Loads and plots training data.
3. Implements cost function and gradient descent.
4. Predicts values for new inputs.
5. Visualizes the cost function surface and contour.

@author Teacher Thales Levi Azevedo Valente
@subject Foundations of Neural Networks
@course Computer Engineering
@university Federal University of Maranh√£o
@date 2025
"""


import numpy as np
import matplotlib.pyplot as plt
import os

from Functions.warm_up_exercises import warm_up_exercise1, warm_up_exercise2, warm_up_exercise3, warm_up_exercise4
from Functions.warm_up_exercises import warm_up_exercise5, warm_up_exercise6, warm_up_exercise7
from Functions.plot_data import plot_data
from Functions.compute_cost import compute_cost
from Functions.gradient_descent import gradient_descent

def experimento_taxas_aprendizado(x_aug, y, iterations):# c√≥digo do experimento 1 aqui
    # === EXPERIMENTO 1: Compara√ß√£o de Taxas de Aprendizado === fiz isso daqui s√≥ pra n√£o ficar t√£o grande o main
    # Esse experimento compara o desempenho do algoritmo de descida do gradiente
    print("\nüìå Experimento 1: Comparando diferentes taxas de aprendizado (Œ±)")

    # Solicita ao usu√°rio tr√™s valores de alpha separados por v√≠rgula
    alphas_input = input("Digite tr√™s valores para a taxa de aprendizado (Œ±), separados por v√≠rgula (ex: 0.001, 0.01, 0.1): ")

    # Converte a string digitada para uma lista de floats
    alphas = [float(val.strip()) for val in alphas_input.split(",") if val.strip()]

    # Cores para os gr√°ficos (at√© 6 alphas suportadas)
    colors = ['r', 'g', 'b', 'c', 'm', 'y']

    # Inicializa theta como vetor de zeros, fixo para todas as compara√ß√µes
    theta_init = np.zeros(2)

    # Cria gr√°fico de converg√™ncia
    plt.figure(figsize=(8, 5))

    # Executa gradient descent para cada alpha fornecido
    for i, alpha_val in enumerate(alphas):
        _, J_hist, _ = gradient_descent(x_aug, y, theta_init.copy(), alpha_val, iterations)
        plt.plot(np.arange(1, iterations + 1), J_hist, colors[i % len(colors)], label=f'Œ± = {alpha_val}')

    # Ajustes de visualiza√ß√£o do gr√°fico
    plt.xlabel('Itera√ß√µes')
    plt.ylabel('Custo J(Œ∏)')
    plt.title('Converg√™ncia da Fun√ß√£o de Custo para Diferentes Taxas de Aprendizado')
    plt.legend()
    plt.grid(True)

    # Salva o gr√°fico como imagem
    plt.savefig("Figures/experimento_taxa_aprendizado.png", dpi=300)
    plt.show()
def experimento_inicializacoes(x_aug, y, iterations, theta0_vals, theta1_vals, j_vals):# c√≥digo do experimento 2 aqui
    # === EXPERIMENTO 2: Compara√ß√£o de Inicializa√ß√µes de Pesos (Œ∏) === Mesma coisa do outro, s√≥ que com inicializa√ß√µes. Fiz assim mais pra n√£o ficar t√£o grande o main, e tbm acho que ficou livre pra gente escolher n√©?
    # Esse experimento compara o desempenho do algoritmo de descida do gradiente
    print("\nüìå Experimento 2: Comparando diferentes inicializa√ß√µes dos pesos Œ∏")

    # Lista para armazenar as inicializa√ß√µes fornecidas
    theta_inputs = []

    # Solicita 3 inicializa√ß√µes manuais (fixas) ao usu√°rio
    print("Digite 3 inicializa√ß√µes fixas para Œ∏, no formato 'Œ∏0 Œ∏1' (ex: 0 0 ou -5 5):")
    for i in range(1, 4):
        entrada = input(f"Inicializa√ß√£o fixa {i}: ")
        try:
            valores = [float(v.strip()) for v in entrada.split()]
            if len(valores) == 2:
                theta_inputs.append(np.array(valores))
            else:
                print("‚ùó Formato inv√°lido. Usando [0, 0] por padr√£o.")
                theta_inputs.append(np.array([0.0, 0.0]))
        except ValueError:
            print("‚ùó Entrada inv√°lida. Usando [0, 0] por padr√£o.")
            theta_inputs.append(np.array([0.0, 0.0]))

    # Adiciona 3 inicializa√ß√µes aleat√≥rias (simulando casos reais de random init)
    for _ in range(3):
        theta_inputs.append(np.random.randn(2))  # distribui√ß√£o normal

    # Cria o gr√°fico de contorno da fun√ß√£o de custo
    plt.figure(figsize=(8, 6))
    plt.contour(theta0_vals, theta1_vals, j_vals, levels=np.logspace(-2, 3, 20))
    plt.xlabel(r'$\theta_0$')
    plt.ylabel(r'$\theta_1$')
    plt.title('Trajet√≥rias do Gradiente para Diferentes Inicializa√ß√µes de Œ∏')
    plt.grid(True)

    # Tra√ßa a trajet√≥ria do gradiente para cada Œ∏ inicial
    for i, init_theta in enumerate(theta_inputs):
        _, _, th_hist = gradient_descent(x_aug, y, init_theta.copy(), alpha=0.01, num_iters=iterations)
        label = f'init {i+1}: {init_theta.round(2)}'
        plt.plot(th_hist[:, 0], th_hist[:, 1], marker='o', markersize=3, label=label)

    # Finaliza e salva o gr√°fico
    plt.legend(fontsize=8)
    plt.savefig("Figures/experimento_inicializacao_pesos.png", dpi=300)
    plt.show()
def main():
    """
    @brief Executa todos os passos do exerc√≠cio de regress√£o linear.

    Esta fun√ß√£o serve como ponto de partida para o exerc√≠cio completo de regress√£o linear.
    Ela executa uma s√©rie de etapas fundamentais, utilizadas como base para o aprendizado
    de modelos supervisionados em redes neurais.

    As principais etapas executadas s√£o:
      1. Executa o exerc√≠cio de aquecimento (warm-up), imprimindo uma matriz identidade 5x5.
      2. Carrega e plota os dados de treinamento de uma regress√£o linear simples.
      3. Calcula o custo com diferentes valores de theta usando a fun√ß√£o de custo J(Œ∏).
      4. Executa o algoritmo de descida do gradiente para minimizar a fun√ß√£o de custo.
      5. Plota a linha de regress√£o ajustada sobre os dados originais.
      6. Realiza previs√µes para valores populacionais de 35.000 e 70.000.
      7. Visualiza a fun√ß√£o de custo J(Œ∏‚ÇÄ, Œ∏‚ÇÅ) em gr√°fico de superf√≠cie 3D e gr√°fico de contorno.

    @instructions
    - Os alunos devem garantir que todas as fun√ß√µes auxiliares estejam implementadas corretamente:
        * warm_up_exercise()
        * plot_data()
        * compute_cost()
        * gradient_descent()
    - Todas as fun√ß√µes devem seguir padr√£o PEP8 e possuir docstrings no formato Doxygen.
    - O script deve ser executado a partir do `main()`.

    @note
    O dataset de entrada `ex1data1.txt` deve estar no mesmo diret√≥rio Data.
    A estrutura esperada dos dados √©: [population, profit].

    @return None
    """
    # Configura√ß√µes do matplotlib para salvar figuras em alta qualidade
    # Garante que a pasta de figuras existe
    os.makedirs("Figures", exist_ok=True)

    print('Executando o exerc√≠cio de aquecimento (warm_up_exercise)...')
    print('Matriz identidade 5x5:')
    # Executa a fun√ß√£o de aquecimento
    # Essa fun√ß√£o deve retornar uma matriz identidade 5x5
    # representada como um array do NumPy.
    # A fun√ß√£o est√° definida em Functions/warm_up_exercise.py
    # e foi importada no in√≠cio deste arquivo.
    print('Executando os exerc√≠cios de aquecimento...')

    # Exerc√≠cio 1: Matriz identidade 5x5
    print('\nExerc√≠cio 1: Matriz identidade 5x5') 
    print(warm_up_exercise1()) # Esperado: [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]]

    # Exerc√≠cio 2: Vetor coluna de 1s
    print('\nExerc√≠cio 2: Vetor de 1s (m=3)')
    print(warm_up_exercise2(3)) # Esperado: [[1], [1], [1]]

    # Exerc√≠cio 3: Adiciona bias ao vetor x
    print('\nExerc√≠cio 3: Adiciona coluna de 1s ao vetor [2, 4, 6]')
    x_ex3 = np.array([2, 4, 6])
    print(warm_up_exercise3(x_ex3)) # Esperado: [[1, 2], [1, 4], [1, 6]]

    # Exerc√≠cio 4: Produto matricial X @ theta
    print('\nExerc√≠cio 4: Produto X @ theta')
    X_ex4 = warm_up_exercise3(x_ex3)
    theta_ex4 = np.array([1, 2])
    print(warm_up_exercise4(X_ex4, theta_ex4))  # Esperado: [5, 9, 13]

    # Exerc√≠cio 5: Erros quadr√°ticos
    print('\nExerc√≠cio 5: Erros quadr√°ticos entre predi√ß√µes e y')
    preds = warm_up_exercise4(X_ex4, theta_ex4)
    y_ex5 = np.array([5, 9, 13])
    print(warm_up_exercise5(preds, y_ex5))  # Esperado: [0, 0, 0]

    # Exerc√≠cio 6: Custo m√©dio a partir dos erros
    print('\nExerc√≠cio 6: Custo m√©dio')
    errors_ex6 = warm_up_exercise5(preds, y_ex5)
    print(warm_up_exercise6(errors_ex6))  # Esperado: 0.0

    # Exerc√≠cio 7: Custo m√©dio com base em X, y e theta
    print('\nExerc√≠cio 7: C√°lculo do custo m√©dio completo')
    print(warm_up_exercise7(X_ex4, y_ex5, theta_ex4))  # Esperado: 0.0


    input("Programa pausado. Pressione Enter para continuar...")

    print('Plotando os dados...')
    # Carrega os dados de treinamento a partir do arquivo ex1data1.txt
    # O arquivo cont√©m duas colunas: a primeira com a popula√ß√£o da cidade
    # (em dezenas de milhar) e a segunda com o lucro (em dezenas de mil d√≥lares).
    # Os dados s√£o carregados usando a fun√ß√£o np.loadtxt do NumPy.
    # A fun√ß√£o np.loadtxt l√™ os dados do arquivo e os armazena em um array NumPy.
    data = np.loadtxt('Data/ex1data1.txt', delimiter=',')
    # Separa os dados em duas vari√°veis: x e y
    # x cont√©m a popula√ß√£o da cidade (em dezenas de milhar)
    # y cont√©m o lucro (em dezenas de mil d√≥lares)
    # A primeira coluna de data √© a popula√ß√£o (x), a feature
    # que ser√° usada para prever o lucro.
    x = data[:, 0] 
    # A segunda coluna de data √© o lucro (y), a label ou target
    y = data[:, 1]
    # A vari√°vel y cont√©m os valores reais de lucro correspondentes
    # Agora, obtemos o n√∫mero de exemplos de treinamento (m)
    m = len(y)

    # Plotagem dos dados
    # Utiliza a fun√ß√£o plot_data para exibir os pontos (x, y) em um gr√°fico 2D.
    # A fun√ß√£o est√° definida em Functions/plot_data.py
    # e foi importada no in√≠cio do arquivo.
    plot_data(x,y) #tinha um espa√ßo meio estranho aqui, mas n√£o sei se era s√≥ impress√£o
    # A fun√ß√£o plot_data recebe como par√¢metros os dados de entrada (x) e os valores de sa√≠da (y).

    input("Programa pausado. Pressione Enter para continuar...")

    # Prepara√ß√£o dos dados para o algoritmo de descida do gradiente
    # Adiciona uma coluna de 1s √† matriz x para representar o termo de intercepta√ß√£o (bias).
    # Isso √© feito com np.column_stack, combinando uma coluna de 1s com os valores de x.
    # A nova matriz x_aug ter√° duas colunas: a primeira com 1s e a segunda com os valores originais de x.
    x_aug = np.column_stack((np.ones(m), x))

    # Inicializa√ß√£o de theta como um vetor nulo (vetor de zeros)
    # Inicializa o vetor de par√¢metros theta como um vetor nulo com 2 elementos (theta[0] e theta[1]).
    # O primeiro elemento representa o intercepto (bias) e o segundo o coeficiente angular (inclina√ß√£o).
    # Esse vetor ser√° ajustado durante a execu√ß√£o do algoritmo de descida do gradiente.
    theta =  np.zeros(2)

    # Par√¢metros da descida do gradiente
    # Define o n√∫mero de itera√ß√µes e a taxa de aprendizado (alpha)
    # O n√∫mero de itera√ß√µes determina quantas vezes os par√¢metros ser√£o atualizados.
    iterations = 1500

    # A taxa de aprendizado (alpha) controla o tamanho do passo dado em cada itera√ß√£o do algoritmo de descida do gradiente.
    # Um alpha muito grande pode fazer o algoritmo divergir, enquanto um muito pequeno pode torn√°-lo lento.
    # Aqui, alpha √© definido como 0.01, um valor comumente usado em problemas de regress√£o linear.
    # Voc√™ pode experimentar outros valores para ver como o algoritmo se comporta.
    alpha = 0.01

    print('\nTestando a fun√ß√£o de custo...')
    # Utiliza a fun√ß√£o compute_cost para calcular o custo com os par√¢metros iniciais (theta = [0, 0]).
    # Essa fun√ß√£o mede o qu√£o bem os par√¢metros atuais se ajustam aos dados de treinamento.
    # Ela est√° definida em Functions/compute_cost.py e foi importada anteriormente.
    # Os par√¢metros de entrada s√£o a matriz x_aug (com 1s e valores de x), o vetor y (lucro) e o vetor theta (par√¢metros).
    cost = compute_cost(x_aug, y, theta)
    print(f'Com theta = [0 ; 0]\nCusto calculado = {cost:.2f}')
    print('Valor esperado para o custo (aproximadamente): 32.07')

    # Testando a fun√ß√£o de custo com outro valor de theta
    # Aqui, testamos a fun√ß√£o de custo com um valor diferente de theta ([-1, 2]).
    # Isso nos permite verificar se a fun√ß√£o de custo est√° funcionando corretamente.
    # O valor de theta = [-1, 2] √© um exemplo e n√£o representa o ajuste ideal.
    cost = compute_cost(x_aug, y, np.array([-1, 2]))
    print(f'\nCom theta = [-1 ; 2]\nCusto calculado = {cost:.2f}')
    print('Valor esperado para o custo (aproximadamente): 54.24')

    input("Programa pausado. Pressione Enter para continuar...")

    print('\nExecutando a descida do gradiente...')
    # Executa o algoritmo de descida do gradiente para encontrar os par√¢metros √≥timos (theta).
    # A fun√ß√£o gradient_descent √© definida em Functions/gradient_descent.py
    # e foi importada anteriormente.
    # Ela recebe como par√¢metros a matriz x_aug (com 1s e valores de x), o vetor y (lucro),

    # Ap√≥s os testes, inicializamos os par√¢metros theta com valores diferentes de zero.
    # theta = [8.5, 4.0] √© o ponto de partida padr√£o. Foi estabelecido empiricamente ao olhar os gr√°ficos.
    # Voc√™ pode experimentar outros valores para ver como o algoritmo se comporta.
    theta = np.array([8.5, 4.0])
    # o vetor theta, a taxa de aprendizado (alpha) e o n√∫mero de itera√ß√µes.
    # A fun√ß√£o retorna os par√¢metros ajustados (theta), o hist√≥rico de custos (J_history) e o hist√≥rico de theta (theta_history).
    # O hist√≥rico de custos √© usado para visualizar a converg√™ncia do algoritmo.
    # O hist√≥rico de theta √© usado para visualizar a trajet√≥ria do gradiente na superf√≠cie da fun√ß√£o de custo.
    theta, J_history, theta_history = gradient_descent(x_aug, y, theta, alpha, iterations)

    print('Par√¢metros theta encontrados pela descida do gradiente:')
    print(theta)
    print('Valores esperados para theta (aproximadamente):')
    print(' -3.6303\n  1.1664')

    # 1. Gr√°fico da converg√™ncia da fun√ß√£o de custo
    plt.figure(figsize=(8, 5))
    plt.plot(np.arange(1, iterations + 1), J_history, 'b-', linewidth=2)
    plt.xlabel('Itera√ß√£o')
    plt.ylabel('Custo J(Œ∏)')
    plt.title('Converg√™ncia da Descida do Gradiente')
    plt.savefig("Figures/convergencia_custo.png", dpi=300, bbox_inches='tight')
    plt.savefig("Figures/convergencia_custo.svg", format='svg', bbox_inches='tight')
    plt.grid(True)
    plt.show()

    # 2. Gr√°fico do Ajuste da Regress√£o Linear
    plt.figure(figsize=(8, 5))
    plt.plot(x, y, 'rx', markersize=5, label='Dados de treino')
    plt.plot(x, x_aug @ theta, 'b-', linewidth=2, label='Regress√£o linear')
    plt.xlabel('Popula√ß√£o da cidade (em dezenas de mil)')
    plt.ylabel('Lucro (em dezenas de mil d√≥lares)')
    plt.title('Ajuste da Regress√£o Linear')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Previs√µes usando theta ajustado
    predict1 = np.array([1, 3.5]) @ theta
    predict2 = np.array([1, 7.0]) @ theta
    print(f'\nPara popula√ß√£o = 35.000, lucro previsto = ${predict1 * 10000:.2f}')
    print(f'Para popula√ß√£o = 70.000, lucro previsto = ${predict2 * 10000:.2f}')
    input("Programa pausado. Pressione Enter para continuar...")

    # 3. Gr√°fico de superf√≠cie 3D da fun√ß√£o de custo J(Œ∏‚ÇÄ, Œ∏‚ÇÅ)
    print('Visualizando a fun√ß√£o J(theta_0, theta_1) ‚Äì superf√≠cie 3D...')
    theta0_vals = np.linspace(-10, 10, 100)
    theta1_vals = np.linspace(-1, 4, 100)
    j_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
    for i, t0 in enumerate(theta0_vals):
        for j, t1 in enumerate(theta1_vals):
            j_vals[i, j] = compute_cost(x_aug, y, np.array([t0, t1]))
    j_vals = j_vals.T

    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')
    t0_mesh, t1_mesh = np.meshgrid(theta0_vals, theta1_vals)
    ax.plot_surface(t0_mesh, t1_mesh, j_vals, cmap='viridis', edgecolor='none')
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel('Custo')
    plt.title('Superf√≠cie da Fun√ß√£o de Custo')
    plt.show()

    # 4. Gr√°fico de contorno da fun√ß√£o de custo
    print('Visualizando a fun√ß√£o J(theta_0, theta_1) ‚Äì contorno...')
    plt.figure(figsize=(8, 5))
    plt.contour(theta0_vals, theta1_vals, j_vals, levels=np.logspace(-2, 3, 20))
    plt.plot(theta[0], theta[1], 'rx', markersize=10, linewidth=2)
    plt.xlabel(r'$\theta_0$')
    plt.ylabel(r'$\theta_1$')
    plt.title('Contorno da Fun√ß√£o de Custo')
    plt.grid(True)
    plt.show()

    # 5) Contorno da fun√ß√£o de custo + trajet√≥ria do gradiente
    plt.figure(figsize=(8, 5))
    # desenha as linhas de contorno
    cs = plt.contour(theta0_vals, theta1_vals, j_vals,
                     levels=np.logspace(-2, 3, 20))
    plt.clabel(cs, inline=1, fontsize=8)  # mostra valores de custo nas linhas

    # sobrep√µe a trajet√≥ria dos thetas
    plt.plot(theta_history[:, 0], theta_history[:, 1],
             'r.-', markersize=6, label='Trajet√≥ria do gradiente')

    plt.xlabel(r'$\theta_0$')
    plt.ylabel(r'$\theta_1$')
    plt.title('Contorno da Fun√ß√£o de Custo com Trajet√≥ria')
    plt.legend()
    plt.grid(True)
    plt.savefig("Figures/contorno_trajetoria.png", dpi=300, bbox_inches='tight')
    plt.savefig("Figures/contorno_trajetoria.svg", format='svg', bbox_inches='tight')
    plt.show()

    # 7) Superf√≠cie da fun√ß√£o de custo com trajet√≥ria 3D melhor visualizada
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')

    # 7.1 Plota a superf√≠cie semitransparente
    surf = ax.plot_surface(
        t0_mesh, t1_mesh, j_vals,
        cmap='viridis',
        edgecolor='none',
        alpha=0.6       # deixa a superf√≠cie semitransparente
    )

    # 7.2 Ajusta √¢ngulo de vis√£o
    ax.view_init(elev=18, azim=-18, roll=-5)

    # 7.3 Trajet√≥ria do gradiente em linha vermelha grossa
    costs = np.concatenate(
        ([compute_cost(x_aug, y, theta_history[0])], J_history)
    )

    # Inserindo a trajet√≥ria 3D do gradiente
    # theta_history: shape (iter+1, 2), J_history: shape (iter,)
    ax.plot(
        theta_history[:, 0], 
        theta_history[:, 1], costs,
        color='red',
        linewidth=3,
        marker='o',
        markersize=4,
        label='Trajet√≥ria do gradiente'
    )

    # 7.4 Destacar ponto inicial e final
    ax.scatter(
        theta_history[0, 0], theta_history[0, 1], costs[0],
        color='blue', s=50, label='In√≠cio'
    )
    ax.scatter(
        theta_history[-1, 0], theta_history[-1, 1], costs[-1],
        color='green', s=50, label='Converg√™ncia'
    )
    
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel('Custo')
    plt.title('Superf√≠cie da Fun√ß√£o de Custo com Trajet√≥ria 3D')
    ax.legend()
    plt.savefig("Figures/superficie_trajetoria.png", dpi=300, bbox_inches='tight')
    plt.savefig("Figures/superficie_trajetoria.svg", format='svg', bbox_inches='tight')
    plt.show()

    #Experimentos aqui:
    # === EXPERIMENTOS COMPARATIVOS INTERATIVOS ===

    # 1. Comparando diferentes taxas de aprendizado (Œ±)
    # ------------------------------------------------
    # C√≥digo completo para solicitar 3 valores de Œ± via input()
    # Executar gradient descent para cada Œ±
    # Plotar gr√°fico comparativo de converg√™ncia
    # Salvar e mostrar o gr√°fico

    # 2. Comparando diferentes inicializa√ß√µes dos pesos (Œ∏)
    # -----------------------------------------------------
    # Solicitar 3 inicializa√ß√µes fixas via input()
    # Adicionar 3 aleat√≥rias
    # Executar gradient descent com cada inicializa√ß√£o
    # Plotar gr√°fico de contorno com trajet√≥rias
    # Salvar e mostrar o gr√°fico
    experimento_taxas_aprendizado(x_aug, y, iterations)
    experimento_inicializacoes(x_aug, y, iterations, theta0_vals, theta1_vals, j_vals)

if __name__ == '__main__':
    main()
